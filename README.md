# credit_card_fraud_detection

This project focuses on identifying fraudulent credit card transactions using both unsupervised anomaly detection methods and a supervised learning model (XGBoost). The dataset is highly imbalanced, making this a great real-world use case to explore the challenges of fraud detection.

üìÇ Dataset
	‚Ä¢	Source: Kaggle ‚Äì Credit Card Fraud Detection
	‚Ä¢	Total transactions: 284,807
	‚Ä¢	Fraudulent transactions: 492 (about 0.17%)
	‚Ä¢	Features: Time, Amount, and PCA-transformed variables (V1‚ÄìV28)
	‚Ä¢	Target: Class (0 = Normal, 1 = Fraud)

üß™ Objective

To compare and evaluate different machine learning models for fraud detection:
	‚Ä¢	Isolation Forest (Unsupervised)
	‚Ä¢	Local Outlier Factor (Unsupervised)
	‚Ä¢	One-Class SVM (Unsupervised)
	‚Ä¢	XGBoost Classifier (Supervised)

üîç Exploratory Data Analysis (EDA)
	‚Ä¢	Checked for missing values (None found)
	‚Ä¢	Visualized class imbalance (highly skewed)
	‚Ä¢	Analyzed transaction amounts for fraud and normal cases
	‚Ä¢	Explored patterns between transaction time and amount

üí° Models and Summary

1. Isolation Forest
	‚Ä¢	Unsupervised, tree-based model
	‚Ä¢	Good for imbalanced data
	‚Ä¢	Recall for fraud: 29%
	‚Ä¢	Accuracy: 99.75%

2. Local Outlier Factor (LOF)
	‚Ä¢	Density-based outlier detection
	‚Ä¢	Poor performance on fraud class
	‚Ä¢	Recall for fraud: 2%
	‚Ä¢	Accuracy: 99.66%

3. One-Class SVM
	‚Ä¢	Learns boundary of normal class
	‚Ä¢	High false positive rate
	‚Ä¢	Recall for fraud: 37%, but 0% precision
	‚Ä¢	Accuracy: 70.09%

4. XGBoost Classifier
	‚Ä¢	Supervised gradient boosting model
	‚Ä¢	Best performance overall
	‚Ä¢	Recall for fraud: 57%
	‚Ä¢	Precision: 80%
	‚Ä¢	Accuracy: 99.93%

‚úÖ Key Observations
	‚Ä¢	XGBoost outperforms all other models in both accuracy and fraud detection.
	‚Ä¢	Isolation Forest is the best among the unsupervised methods.
	‚Ä¢	LOF and SVM are not suitable for this highly imbalanced problem without additional tuning.
	‚Ä¢	Precision-recall trade-off is crucial due to the rarity of fraudulent transactions.

üìå Final Notes
	‚Ä¢	Class imbalance is a significant challenge in fraud detection.
	‚Ä¢	Anomaly detection models work without labels but have limited precision.
	‚Ä¢	XGBoost, with labeled data, provides high accuracy and reasonable fraud detection performance.
	‚Ä¢	This project shows that combining EDA, sampling, and choosing the right algorithm can significantly improve model outcomes.
